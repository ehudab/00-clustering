{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff93b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-aaab2a85-01fd-46fb-89d4-646f38c26b3d\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-aaab2a85-01fd-46fb-89d4-646f38c26b3d\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "\u001b[1;31mrequest to https://8080-m-s-2yh6ae73xe6v4-c.us-west1-2.prod.colab.dev/api/kernels/0c98f7fb-5da6-4943-82ae-cfd8d2648032/restart?1772054981144 failed, reason: socket hang up. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#!pip install numpy scikit-learn matplotlib pandas\n",
    "#from google.colab import files\n",
    "\n",
    "# Upload files\n",
    "#uploaded = files.upload()\n",
    "\n",
    "# The uploaded files will be saved in the current working directory\n",
    "# You can access them using their filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8598d32",
   "metadata": {},
   "source": [
    "# Clustering From Scratch\n",
    "Applying K-Means, DBSCAN, and HDBSCAN to the provided dataset using only NumPy, Pandas, and Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('clustering.csv')\n",
    "X = data[['x', 'y']].to_numpy()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(X):\n",
    "    diff = X[:, None, :] - X[None, :, :]\n",
    "    return np.sqrt(np.sum(diff * diff, axis=2))\n",
    "\n",
    "def plot_clusters(X, labels, title):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "    for color, lbl in zip(colors, unique_labels):\n",
    "        mask = labels == lbl\n",
    "        if lbl == -1:\n",
    "            plt.scatter(X[mask, 0], X[mask, 1], s=10, c=[color], marker='x', label='noise')\n",
    "        else:\n",
    "            plt.scatter(X[mask, 0], X[mask, 1], s=10, c=[color], label=f'cluster {lbl}')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best', fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628b634",
   "metadata": {},
   "source": [
    "## K-Means from Scratch\n",
    "Simple iterative centroid-based clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf2b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansScratch:\n",
    "    def __init__(self, k, max_iters=100, random_state=0):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.random_state = random_state\n",
    "        self.centroids = None\n",
    "\n",
    "    def initialize_centroids(self, X):\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        indices = rng.choice(len(X), size=self.k, replace=False)\n",
    "        self.centroids = X[indices].copy()\n",
    "\n",
    "    def assign_clusters(self, X):\n",
    "        distances = np.linalg.norm(X[:, None, :] - self.centroids[None, :, :], axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def update_centroids(self, X, labels):\n",
    "        new_centroids = []\n",
    "        for idx in range(self.k):\n",
    "            points = X[labels == idx]\n",
    "            if len(points) == 0:\n",
    "                new_centroids.append(self.centroids[idx])\n",
    "            else:\n",
    "                new_centroids.append(points.mean(axis=0))\n",
    "        self.centroids = np.vstack(new_centroids)\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        self.initialize_centroids(X)\n",
    "        for _ in range(self.max_iters):\n",
    "            labels = self.assign_clusters(X)\n",
    "            old_centroids = self.centroids.copy()\n",
    "            self.update_centroids(X, labels)\n",
    "            if np.allclose(old_centroids, self.centroids):\n",
    "                break\n",
    "        return labels\n",
    "\n",
    "kmeans = KMeansScratch(k=4, max_iters=50, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "plot_clusters(X, kmeans_labels, 'K-Means Clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521ff71",
   "metadata": {},
   "source": [
    "## DBSCAN from Scratch\n",
    "Density-based clustering that marks low-density points as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCANScratch:\n",
    "    def __init__(self, eps, min_samples):\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "\n",
    "    def region_query(self, X, idx):\n",
    "        distances = np.linalg.norm(X - X[idx], axis=1)\n",
    "        return np.where(distances <= self.eps)[0]\n",
    "\n",
    "    def expand_cluster(self, X, labels, idx, cluster_id):\n",
    "        seeds = list(self.region_query(X, idx))\n",
    "        if len(seeds) < self.min_samples:\n",
    "            labels[idx] = -1\n",
    "            return False\n",
    "        labels[idx] = cluster_id\n",
    "        i = 0\n",
    "        while i < len(seeds):\n",
    "            point = seeds[i]\n",
    "            if labels[point] == -1:\n",
    "                labels[point] = cluster_id\n",
    "            if labels[point] != 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            labels[point] = cluster_id\n",
    "            neighbors = self.region_query(X, point)\n",
    "            if len(neighbors) >= self.min_samples:\n",
    "                seeds.extend([n for n in neighbors if n not in seeds])\n",
    "            i += 1\n",
    "        return True\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        labels = np.zeros(len(X), dtype=int)\n",
    "        cluster_id = 0\n",
    "        for idx in range(len(X)):\n",
    "            if labels[idx] != 0:\n",
    "                continue\n",
    "            if self.expand_cluster(X, labels, idx, cluster_id + 1):\n",
    "                cluster_id += 1\n",
    "        labels[labels == 0] = -1\n",
    "        return labels  # keep noise at -1, clusters start at 1; adjust to start at 0 below\n",
    "\n",
    "dbscan = DBSCANScratch(eps=25.0, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X)\n",
    "dbscan_labels = np.where(dbscan_labels == -1, -1, dbscan_labels - 1)\n",
    "plot_clusters(X, dbscan_labels, 'DBSCAN Clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb5434",
   "metadata": {},
   "source": [
    "## HDBSCAN from Scratch (simplified)\n",
    "Hierarchy over density via mutual reachability distances and a stability-based cluster selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5eb88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def core_distances(X, min_samples):\n",
    "    \"\"\"Return core distance for each point: distance to its (min_samples)-th nearest neighbor.\n",
    "    X: (n, d) array, min_samples: int\n",
    "    \"\"\"\n",
    "    dists = pairwise_distances(X)                    # full pairwise distance matrix (n,n)\n",
    "    sorted_dists = np.sort(dists, axis=1)            # sort distances for each point\n",
    "    # index min_samples-1 to keep same convention as earlier (includes self at index 0)\n",
    "    return sorted_dists[:, min_samples - 1]          # shape (n,)\n",
    "\n",
    "def mutual_reachability_distances(X, min_samples):\n",
    "    \"\"\"Compute mutual reachability distance matrix:\n",
    "    mreach(i,j) = max( pairwise(i,j), core(i), core(j) )\n",
    "    \"\"\"\n",
    "    core = core_distances(X, min_samples)            # core distance per point, shape (n,)\n",
    "    pairwise = pairwise_distances(X)                 # pairwise distances, shape (n,n)\n",
    "    # broadcast core to (n,n) and take elementwise maximum\n",
    "    return np.maximum(pairwise, np.maximum(core[:, None], core[None, :]))\n",
    "\n",
    "def prim_mst(mreach):\n",
    "    \"\"\"Prim's algorithm on a dense symmetric matrix to return MST edges as (u,v,weight).\n",
    "    mreach: (n,n) mutual reachability matrix\n",
    "    \"\"\"\n",
    "    n = mreach.shape[0]\n",
    "    selected = np.zeros(n, dtype=bool)\n",
    "    selected[0] = True                                # start from node 0\n",
    "    min_dist = mreach[0].copy()                       # best distance from selected set\n",
    "    min_edge = np.zeros(n, dtype=int)                 # keeps nearest selected neighbor for each node\n",
    "    edges = []\n",
    "    for _ in range(n - 1):\n",
    "        min_dist[selected] = np.inf                   # ignore already selected nodes\n",
    "        v = np.argmin(min_dist)                       # next node to add\n",
    "        u = min_edge[v]                               # its nearest selected neighbor\n",
    "        w = min_dist[v]                               # edge weight\n",
    "        edges.append((u, v, w))\n",
    "        selected[v] = True\n",
    "        # update distances/edges using newly selected vertex v\n",
    "        for j in range(n):\n",
    "            if not selected[j] and mreach[v, j] < min_dist[j]:\n",
    "                min_dist[j] = mreach[v, j]\n",
    "                min_edge[j] = v\n",
    "    return edges                                     # list length n-1\n",
    "\n",
    "def build_cluster_tree(n_points, edges):\n",
    "    \"\"\"Build a hierarchical merge tree from MST edges sorted by weight.\n",
    "    Returns root id, children dict, and stability dict for nodes.\n",
    "    Node ids < n_points are original points, >= n_points are internal merge nodes.\n",
    "    \"\"\"\n",
    "    edges_sorted = sorted(edges, key=lambda x: x[2])  # sort by increasing weight\n",
    "    next_id = n_points\n",
    "    parent = {i: i for i in range(n_points)}         # union-find like parent mapping\n",
    "    size = {i: 1 for i in range(n_points)}           # cluster size (# original points)\n",
    "    birth = {i: 0.0 for i in range(n_points)}        # birth lambda for leaf: 0.0\n",
    "    death = {i: None for i in range(n_points)}       # death lambda when merged\n",
    "    children = {i: [] for i in range(n_points)}      # child list for each node\n",
    "\n",
    "    def find(x):\n",
    "        # path-compress find for current parent mapping\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "\n",
    "    for u, v, w in edges_sorted:\n",
    "        ru, rv = find(u), find(v)\n",
    "        if ru == rv:\n",
    "            continue\n",
    "        lambda_val = 1.0 / w if w > 0 else np.inf     # convert distance -> 'lambda' (density)\n",
    "        # create new internal node representing merge(ru,rv)\n",
    "        parent[ru] = next_id\n",
    "        parent[rv] = next_id\n",
    "        parent[next_id] = next_id\n",
    "        children[next_id] = [ru, rv]\n",
    "        birth[next_id] = lambda_val\n",
    "        death[ru] = lambda_val\n",
    "        death[rv] = lambda_val\n",
    "        size[next_id] = size[ru] + size[rv]\n",
    "        # zero-out sizes of merged children to avoid double-counting later\n",
    "        size[ru] = 0\n",
    "        size[rv] = 0\n",
    "        next_id += 1\n",
    "\n",
    "    # find root (representative of node 0)\n",
    "    root = find(0)\n",
    "    # set death of root to 0 (it persists down to zero density)\n",
    "    death[root] = 0.0\n",
    "\n",
    "    # compute stability for nodes that have a death value\n",
    "    stability = {}\n",
    "    for cid in list(birth.keys()) + [i for i in range(n_points, next_id)]:\n",
    "        b = birth.get(cid, 0.0)\n",
    "        d = death.get(cid, None)\n",
    "        s = max(size.get(cid, 0), 0)\n",
    "        if d is not None:\n",
    "            # stability = (death - birth) * size (how long cluster persists weighted by size)\n",
    "            stability[cid] = (d - b) * max(s, 1)\n",
    "    return root, children, stability\n",
    "\n",
    "def select_clusters(root, children, stability):\n",
    "    \"\"\"Select stable clusters from the hierarchical tree using a simple greedy rule:\n",
    "    pick a node if its stability >= sum(stabilities of selected children), else use children.\n",
    "    Returns list of selected node ids.\n",
    "    \"\"\"\n",
    "    def recurse(node):\n",
    "        # leaf node -> nothing to select at internal level\n",
    "        if node not in children or len(children[node]) == 0:\n",
    "            return []\n",
    "        selected_children = []\n",
    "        for child in children[node]:\n",
    "            selected_children.extend(recurse(child))\n",
    "        # compute scores\n",
    "        child_score = sum(stability.get(c, 0.0) for c in selected_children)\n",
    "        self_score = stability.get(node, 0.0)\n",
    "        if self_score >= child_score and node in stability:\n",
    "            return [node]\n",
    "        return selected_children\n",
    "    return recurse(root)\n",
    "\n",
    "def label_points(n_points, selected, children):\n",
    "    \"\"\"Assign final cluster labels to original points given selected cluster nodes.\n",
    "    Points not covered by any selected node remain noise (-1).\n",
    "    \"\"\"\n",
    "    labels = np.full(n_points, -1, dtype=int)\n",
    "    cluster_id = 0\n",
    "\n",
    "    def assign(node, cid):\n",
    "        # if node is an original point, label it\n",
    "        if node < n_points:\n",
    "            labels[node] = cid\n",
    "            return\n",
    "        # otherwise, recurse into children\n",
    "        for child in children.get(node, []):\n",
    "            assign(child, cid)\n",
    "\n",
    "    for node in selected:\n",
    "        assign(node, cluster_id)\n",
    "        cluster_id += 1\n",
    "    return labels\n",
    "\n",
    "def hdbscan_scratch(X, min_samples=5):\n",
    "    \"\"\"Top-level simplified HDBSCAN: compute mutual reachability, MST, build tree,\n",
    "    select clusters by stability, and label points.\n",
    "    \"\"\"\n",
    "    mreach = mutual_reachability_distances(X, min_samples)   # (n,n)\n",
    "    edges = prim_mst(mreach)                                 # MST edges (n-1)\n",
    "    root, children, stability = build_cluster_tree(len(X), edges)\n",
    "    selected = select_clusters(root, children, stability)\n",
    "    labels = label_points(len(X), selected, children)\n",
    "    return labels\n",
    "\n",
    "# run simplified HDBSCAN (adjust min_samples as needed)\n",
    "hdbscan_labels = hdbscan_scratch(X, min_samples=3)\n",
    "plot_clusters(X, hdbscan_labels, 'HDBSCAN Clusters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4ce2b",
   "metadata": {},
   "source": [
    "## Quick Comparison\n",
    "Basic summary stats for the three runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_summary(labels):\n",
    "    counts = pd.Series(labels).value_counts().sort_index()\n",
    "    n_clusters = (counts.index != -1).sum()\n",
    "    n_noise = counts.get(-1, 0)\n",
    "    return n_clusters, n_noise, counts.to_dict()\n",
    "\n",
    "k_summary = cluster_summary(kmeans_labels)\n",
    "d_summary = cluster_summary(dbscan_labels)\n",
    "h_summary = cluster_summary(hdbscan_labels)\n",
    "\n",
    "print('K-Means clusters:', k_summary[0])\n",
    "print('DBSCAN clusters:', d_summary[0], 'noise:', d_summary[1])\n",
    "print('HDBSCAN clusters:', h_summary[0], 'noise:', h_summary[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41225c1a",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Simple internal metrics (silhouette, Davies-Bouldin, Calinski-Harabasz). Noise points (-1) are ignored for the metrics that require cluster membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "def score_all(X, labels):\n",
    "    mask = labels >= 0\n",
    "    Xc = X[mask]\n",
    "    lc = labels[mask]\n",
    "    unique = np.unique(lc)\n",
    "    if len(Xc) == 0 or len(unique) < 2:\n",
    "        return {\n",
    "            'silhouette': np.nan,\n",
    "            'davies_bouldin': np.nan,\n",
    "            'calinski_harabasz': np.nan\n",
    "        }\n",
    "    silhouette = silhouette_score(Xc, lc, sample_size=min(len(Xc), 4000), random_state=0) if len(Xc) > 50 else silhouette_score(Xc, lc)\n",
    "    db = davies_bouldin_score(Xc, lc)\n",
    "    ch = calinski_harabasz_score(Xc, lc)\n",
    "    return {'silhouette': float(silhouette), 'davies_bouldin': float(db), 'calinski_harabasz': float(ch)}\n",
    "\n",
    "metrics = {\n",
    "    'kmeans': score_all(X, kmeans_labels),\n",
    "    'dbscan': score_all(X, dbscan_labels),\n",
    "    'hdbscan': score_all(X, hdbscan_labels)\n",
    "}\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
